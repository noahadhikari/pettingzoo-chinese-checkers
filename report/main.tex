% main.tex
\documentclass[12pt, a4paper, twocolumn]{article}


% half inch margins
\usepackage{geometry}
\geometry{left=0.5in,right=0.5in,top=0.5in,bottom=0.5in}


\title{Title}

\author{Allen Gu and Noah Adhikari}


\date{
    University of California, Berkeley\\[2ex] % affiliation hack
    December 13, 2023    
}

\newcommand{\abstractText}{\noindent
Abstract goes here.
}

%%%%%%%%%%%%%%%%%
% Configuration %
%%%%%%%%%%%%%%%%%

\usepackage{xurl}
\usepackage[super,comma,sort&compress]{natbib}
\usepackage{abstract}
\renewcommand{\abstractnamefont}{\normalfont\bfseries}
\renewcommand{\abstracttextfont}{\normalfont\small\itshape}
\usepackage{lipsum}


% Any configuration that should be done before the end of the preamble:
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor=blue, linkcolor=blue, citecolor=blue}


\usepackage{array}
\usepackage{listings}
\usepackage[noline,ruled]{algorithm2e}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{multicol}

\usepackage{etoolbox}
\preto\align{\par\nobreak\small\noindent}
\expandafter\preto\csname align*\endcsname{\par\nobreak\small\noindent}

\begin{document}

%%%%%%%%%%%%
% Abstract %
%%%%%%%%%%%%

\twocolumn[
  \begin{@twocolumnfalse}
    \maketitle
    \begin{abstract}
      \abstractText
      \newline
      \newline
    \end{abstract}
  \end{@twocolumnfalse}
]

%%%%%%%%%%%
% Article %
%%%%%%%%%%%

\section{Introduction}

Multi-agent reinforcement learning (MARL) has been a growing topic of interest in recent years \cite{ArticleReference1}. Most prior work in this area has focused on two-player competitive, zero-sum games such as Go \cite{ArticleReference2} or positive-sum cooperative games\cite{ArticleReference3}. Several frameworks and tools have been built to support these environments \cite{PettingZoo} \cite{Gym}. 

The behavior of agents in competitive environments with more than two players is much less well studied \cite{ArticleReference4}. Stochastic games such as poker are popular in this domain \cite{ArticleReference5}, but complete-information environments are not as well studied. In this paper, we develop a custom PettingZoo environment for Chinese Checkers, a six-player competitive homogenous game, as well as study the performance of several MARL architectures on this environment.


% chinese checkers citation 1 https://cs229.stanford.edu/proj2016spr/report/003.pdf
% chinese checkers citation 2 https://arxiv.org/pdf/1903.01747.pdf
Though reinforcement learning for Chinese Checkers has been explored before \cite{ArticleReference6}, our implementation is, to the best of our knowledge, the first to remain faithful to the true game.

\section{Background}

\subsection{Chinese Checkers Rules}

Chinese Checkers is a game traditionally played with 2, 3, 4, or 6 players of differing colors on a star-shaped board as seen in Figure \ref{fig:board}. Each player traditionally has 10 pegs, which are initially placed in the corner of the board closest to them. The goal of the game is to move all of one's pegs to the corner of the board opposite to their starting corner.

\subsubsection{Movement}
The game is played in turns. On a player's turn, they must move one of their pegs to an adjacent empty space, or jump over an adjacent peg of any color to an empty space on the other side. A player may jump over multiple pegs in a single turn in a process known as ``chaining'' \cite{ChainingJumps}. A player can only move their own pegs. Only if a player has no possible moves may they pass their turn entirely.

\subsubsection{Spoiling}
It is possible for a player to prevent another from winning by continously occupying one of the pegs in their goal corner, in a process known as ``spoiling.'' \cite{ChineseCheckersSpoiling} There are no official rules preventing spoiling, and rules prohibiting it tend to be quite complicated, so we permit it in our implementation.

\subsubsection{Winning}
The first player to move all of their pegs to the opposite corner wins the game. Though it is possible to continue playing after this point ranking purposes, we focus on the variant where the game is terminated after the first player wins.

\subsection{Hexagonal Coordinate Systems}

The geometry of Chinese Checkers lends itself nicely to a hexagonal grid, but it is nontrivial to represent hexagonal grids, and there are various ways to do so. \cite{HexagonalCoordinates}

One way involves exploiting a symmetry between cubes and hexagons, where each axis of the cube corresponds to the projection in 2D-space of one of the three axes of the hexagon. One can then describe the position of a hexagon in terms of three coordinates $(q, r, s)$. Since the hexagonal grid lies in a plane, however, the coordinates are subject to the constraint $q + r + s = 0$. This is known as the cube coordinate system.

Because of the planar constraint, only two of the three $qrs$-coordinates are necessary. $(q, r)$ is a common choice; this is known as the axial coordinate system.

Figure \ref{fig:hex} shows the relationship between the two coordinate systems.

\subsection{Proximal Policy Optimization (PPO)}

Proximal Policy Optimization\cite{PPO} (PPO) is an on-policy method with an architecture similar to actor-critic algorithms. However, it addresses stability concerns in traditional actor-critic algorithms by limiting large policy updates. The algorithm accomplishes this through a clipped surrogate objective.

% TODO: HELP idk how to make the font smaller :'( %
\begin{align*}
  L = \mathbb{E}[\min(\frac{\pi(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)} \hat{A_t}, \text{clip}(\frac{\pi(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}, 1 - \epsilon, 1 + \epsilon) \hat{A_t})]
\end{align*}

In the objective, the advantage terms incentivizes taking actions that are better than average. The ratio $\frac{\pi(a_t | s_t)}{\pi_{\text{old}}(a_t | s_t)}$ is a measure of how far the new policy deviates from the previous one. By clipping and minimizing with the non-clipped ratio, the objective also deters making large changes to the policy. 

\section{Methods}

\subsection{Chinese Checkers Environment}

We developed a custom PettingZoo environment for Chinese Checkers to support MARL. The board has the same shape as the traditional game, but can be configured to play with different sizes $N$. For example, the $N = 2$ variation that we used for many experiments has 3 pegs per player. In axial coordinates, this board can be represented by a 2D array of size $(4N + 1, 4N + 1)$ because an axis through the center of the board has $4N + 1$ slots.

\begin{figure}
  \centering
    \includegraphics[width=0.5\textwidth]{images/boards.png}
  \caption{Game boards of size $N = 2, 3, 4$}
  \label{fig:boards}
\end{figure}

\subsubsection{Number of Players}

For simplicity, we focus on the 6-player variant of Chinese Checkers as we wanted to investigate several algorithms in multiplayer self-play. However, our methods can be applied to other numbers of players.

\subsubsection{Turn Limits}

Unlike games such as chess, which impose turn limits to prevent infinite horizons \cite{ChessTurnLimit}, Chinese Checkers has no such rules. We impose a turn limit that ends the game with no winners after a large, configurable amount of turns (we found $\approx 1000$ to be a good choice for $N=4$).

\subsubsection{Submoves}

In Chinese Checkers, it may be difficult to enumerate all of an agent's possible moves, as jumps increase the number of possible moves significantly. Additionally, within a single move, it is possible to infinitely stall the game by jumping in a cycle around one or more pegs as shown in Figure \ref{fig:cycle}.

To prevent this, we adopt the concept of submoves from RL checkers environments \cite{RLCheckers}, where a move consists of one or more submoves that may not end the player's turn immediately. In Chinese Checkers, a submove is a single jump, adjacent-cell movement, or end-turn action. Jump submoves may not necessarily end a player's turn, but to prevent infinite turns, players cannot jump back to a cell they previously occupied on that turn. Players can end their turn following a jump using the end-turn submove.

\subsubsection{Board Representation}
We represent variable board size as $N$, where $N$ represents the side lengths of one of the triangular corners of the board (so $N=4$ is traditional Chinese Checkers). The game board uses cube coordinates, with the center of the board being $(q, r, s) = (0, 0, 0)$.

Note that due to the star-shaped nature of the board, it is difficult to represent the board as a perfect hexagon. To circumvent this, we simply treat the board as a hexagon of side length $(4N+1)$, which corresponds to a circumscibed hexagon about the star. Extraneous cells are then masked out. We assign integer values to cells as follows:

\begin{center}
  \begin{tabular}{|m{0.05\textwidth}|m{0.35\textwidth}|}
    \hline
    Value & Description \\
    \hline
    -2 & Out-of-bounds \\
    -1 & Empty \\
    0 & Player 0 (red peg) \\
    1 & Player 1 (black peg) \\
    2 & Player 2 (yellow peg) \\
    3 & Player 3 (green peg) \\
    4 & Player 4 (blue peg) \\
    5 & Player 5 (white peg) \\
    \hline
  \end{tabular}
  \end{center}

For symmetry, we rotate the board so that the current player's home triangle is at the top of the board when making moves. 
  
The 3D representation of the board is quite sparse, so we flatten the board with the axial coordinate constraint $q + r + s = 0$ when communicating with agents.

\subsubsection{Observation Space}

The observation space is a flattened representation of 8 different layers of the board, where each layer provides unique information about the game state. Each layer is as a two-dimensional $(4N + 1, 4N + 1)$ grid where row and column indices $(i, j)$ map to axial coordinates $(q, r)$ of the board space. To maintain rotational symmetry, the board is always rotated when creating the observation so that the current player's home triangle is at the top of the board. The layers represent the following binary information for each space on the board:

\begin{center}
\begin{tabular}{|m{0.05\textwidth}|m{0.35\textwidth}|}
  \hline
  Layer & Description \\
  \hline
  0 & Current player's pegs \\
  1-5 & Other players' pegs, in clockwise order starting from the current player \\
  6 & Where the current player has jumped from this turn \\
  7 & Where the current player jumped during the last submove, if the last submove was a jump \\
  \hline
\end{tabular}
\end{center}

After flattening, the observation space becomes a one-dimensional $(4N + 1) \times (4N + 1) \times 8$ binary vector.

% \vfill

\subsubsection{Action Space}

Actions in the Chinese Checkers environment are represented by a tuple $(q, r, \texttt{direction}, \texttt{is\_jump})$:

\begin{center}
  \begin{tabular}{|m{0.1\textwidth}|m{0.3\textwidth}|}
    \hline
    $(q, r)$ & Position of the peg being moved in axial coordinates \\
    \hline
    \texttt{direction} & Direction of movement from $0$ to $6$ in angle increments of $\frac{\pi}{3}$, where $0$ is due-right, $1$ is up-right, etc. \\
    \hline
    \texttt{is\_jump} & Whether the action is a jump \\
    \hline
  \end{tabular}
  \end{center}

  If a previous submove was a jump or if no legal moves are available, a player may also end their turn immediately without a move (known as the end-turn action)  Thus, we represent the action space as a discrete space of size $(4N + 1) \times (4N + 1) \times 6 \times 2 + 1$.

\subsubsection{Action Masking}

For each agent, the observation, in addition to the observation space, also includes an attribute \texttt{action\_mask} which is a binary vector of size $(4N + 1) \times (4N + 1) \times 6 \times 2 + 1$ that indicates which actions are legal for the current agent during the current submove. This is used to mask out illegal actions when sampling from the action space.

\subsubsection{Rewards}

Various reward schemes were tried:

\begin{enumerate}
  \item \textit{Sparse:} $+5N$ for winning, $-N$ for losing, $0$ otherwise
  \item Sparse reward, plus small \textit{goal bonuses} for entering pegs in the goal ($+0.1$) and penalties for removing them from the goal ($-0.1$)
  \item Sparse reward, plus tiny \textit{movement bonuses} for moving forward ($+0.001$) and a small penalty for moving backward ($-0.001$)
  \item \textit{Positive-sum:} $+5N$ for winning, $0$ otherwise, plus goal and movement bonuses
\end{enumerate}

We found that agents with sparse rewards had difficulty learning, so we ultimately opted for a positive sum, dense reward scheme (4) due to our goals involving self-play.

\subsubsection{Environment Stepping}

The pseudocode for a single environment step within our game is summarized below. Each step corresponds to moving a single peg within the game or ending a turn. Consecutive jumps are handled across multiple environment steps.

\begin{algorithm}
  \SetKwInOut{Input}{Input}

  \textbf{function} step (a)
  
  \Indp

  \Input{\textit{a}: Integer action in $[0, (4N + 1) \times (4N + 1) \times 6 \times 2 + 1)$}
  
  Convert integer action to a submove

  Rotate the board to the current player and perform the submove

  Update rewards for agents

  \If{submove is not a jump}
    {
      Advance to the next player
    }

  \Indm

  \textbf{end}

  \caption{Environment stepping in Chinese Checkers}
\end{algorithm}
  
\subsection{Multi-Agent Learning}

There are many approaches to configuring models in multi-agent settings. In this section, we describe both our model algorithm and the variations we tested on the Chinese Checkers environment.

\subsubsection{Model Architecture}

All agents learn through Proximal Policy Optimization (PPO) with a clipped surrogate objective\cite{PPO}. In our architecture, the actor and critic share an MLP encoder but have differing policy and value function heads.

\textbf{Encoder Network:} The encoder network takes in a vector the size of the observation space as input. Each encoder contains a hidden layer of size 64 and output layer of size 64, with ReLU activation functions.

\textbf{Policy Head:} The policy head is a single fully connected network of shape $(64, 12(4N + 1)^2 + 1)$ that takes the encoder output as input and outputs action logits.

\textbf{Value Function Head:} The value function head is a single fully connected network of shape $(64, 1)$ that takes the encoder output as input and outputs a scalar estimating the value of the state.

\subsubsection{Variations on Multi-Agent Setup}

We compare three different policy configurations for the multiple agents: fully-independent, shared-encoder, and fully-shared variations.
\begin{itemize}
  \item \textit{Fully-Independent:} Agents learn separately with their own single-agent PPO policy. 
  \item \textit{Shared-Encoder:} All agents share the same encoder, but have unique policy and value function heads.
  \item \textit{Fully-Shared:} Agents share all parameters, effectively training a single policy.
\end{itemize}

\subsubsection{Training Procedure}

% TODO: UPDATE CURRICULUM LEARNING THINGY WITH THIS THINGY https://arxiv.org/pdf/2202.10608.pdf
We trained all three policy configurations using self-play. As the policies are updated, the agents become stronger and thus have stronger opposition, so this algorithm is reminiscient of curriculum learning \cite{CurriculumLearning}. The training algorithm is summarized below:

% self.lr_schedule = None
%         self.lr = 5e-5
%         self.rollout_fragment_length = "auto"
%         self.train_batch_size = 4000

%         # PPO specific settings:
%         self.use_critic = True
%         self.use_gae = True
%         self.lambda_ = 1.0
%         self.use_kl_loss = True
%         self.kl_coeff = 0.2
%         self.kl_target = 0.01
%         self.sgd_minibatch_size = 128
%         self.num_sgd_iter = 30
%         self.shuffle_sequences = True
%         self.vf_loss_coeff = 1.0
%         self.entropy_coeff = 0.0
%         self.entropy_coeff_schedule = None
%         self.clip_param = 0.3
%         self.vf_clip_param = 10.0
%         self.grad_clip = None


\begin{algorithm}
  \SetKwInOut{Input}{Input}
  \SetKwFor{RepTimes}{repeat}{times}{end}

  \textbf{function} train (n, policy)

  \Indp
  
  \Input{\textit{n}: Number of training iterations \\
         \textit{policy}: Policy configuration}
  
  Initialize 6 policies following the given configuration, sharing parameters if necessary

  \RepTimes{n} {
    Sample a batch of experiences by playing against the other policies

    Update the policies using PPO
  }

  \Indm
  \textbf{end}
  \caption{Agent self-play training procedure}
\end{algorithm}


\subsubsection{Evaluation Procedures}

We evaluate the performance of trained agents via several metrics.

\textit{Random agent evaluation}: The evaluated agent plays against a field of random agents. The number of wins and the number of turns taken to win are both recorded and can be used for evaluation purposes.

\begin{algorithm}
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
  \SetKwFor{RepTimes}{repeat}{times}{end}

  \textbf{function} evaluate (n, policy)

  \Indp
  
  \Input{\textit{n}: Number of evaluation games \\
         \textit{policy}: Policy configuration}
  \Output{\textit{wins}: Number of wins during evaluation play \\
          \textit{turns}: Average number of turns to win}
  
  Initialize player 0 to follow the given policy
  
  Initialize players 1-5 to follow a random policy

  \RepTimes{n} {
    Play a game with the given policies

    Record whether the game was a win

    Record the number of turns taken
  }

  \Indm
  \textbf{end}
  \caption{Evaluation procedure against random agents}
\end{algorithm}

\textit{Team matches}: Each of the three architectures contributes two agents in a match of six players. The positions of the agents are randomized, and the agents play a game against each other. The number of wins and the number of turns taken to win are both recorded and can be used for evaluation purposes.




\section{Experiments}

\subsection{Comparison of Multi-Agent Configurations}

First, we compare the different multi-agent setups: fully-independent, shared-encoder, and fully-shared. Each algorithm was trained over 100 iterations of 4000 timesteps each, with evaluation after every timestep. During evaluation, one trained policy would compete for 30 rounds against five random policies until one player wins or 150 turns have been played.

% TODO: how do you make these two figures go side by side across columns instead of on one column %
\begin{figure}[ht]
  \centering
    \includegraphics[width=0.47\textwidth]{images/winrate.png}
    \includegraphics[width=0.47\textwidth]{images/game_length.png}
  \label{fig:winrate}
  \caption{Win rate when evaluated against five random policies. Evaluation occurred at repeated intervals throughout training for fully-independent, shared-encoder, and fully-shared multi-agent configurations.}
\end{figure}

From the results in Figure \ref{fig:winrate}, we find that as more parameters are shared between the agents, the more efficient our training becomes. Full parameter sharing reached over a 90\% winrate in less than 50,000 environment steps of training while the fully-independent algorithm took almost 200,000 environment steps.

Additionally, the game durations during evaluation were shorter for parameter-sharing methods. The policy trained through a fully-shared approach was able to win in fewer turns, where one turn consists of all moves made by a single player before allowing the next player to act. From Figure \ref{fig:game_length}, full parameter sharing converges to a policy that wins in nearly 20 moves, faster than the other two approaches.

A naive strategy for Chinese checkers could be to move each piece from the home triangle to the target triangle one by one, without jumps. For a board of size $N = 2$, this strategy performs 20 moves. The optimal strategy including jumps would improve this by a couple moves. Interestingly, even when facing opponents that may both harm (e.g. block the target triangle) or help (e.g. increase the potential for jumps), our model is near this hypothetical optimum.

\subsection{Training Convergence}

Although all trained models eventually reach a 100\% win-rate against random players, the models can still improve their strategies. For example, all models seem to move the pieces to the target zone in an independent fashion: moving one piece first, then the next, and so on. A better approach could be to collaborate and create jump opportunities for the other pegs to advance.

% TODO: make this one big too %
% TODO: this plot should probably use the same color scale %

\begin{figure}[ht]
  \centering
    \includegraphics[width=0.47\textwidth]{images/random_heatmap.png}
  \label{fig:heatmap}
  \caption{Heatmaps for the policy trained through full parameter sharing. These plots show the frequency of different peg locations on the board after a varying amount of turns have been made.}
\end{figure}

To understand the strategy that policies learn in training, various checkpoints from training were run 100 times against five random opponents. During each game, the board states were collected at different numbers of turns. These board states were used to generate heat maps showing the frequency of pegs throughout the board.

This behavior seems to be learned early on in training, as shown by Figure \ref{fig:heatmap}. By the 10th iteration, we see in later turns like turn 15 that there is a high probability that some pegs are still in the home triangle while others have already made it to the target triangle. A similar pattern can still be seen on the 99th training iteration. Although we see it earlier, by turn 5, many pegs are still left in the home triangle while others are far down the board.



\section{Discussion}

\lipsum[1]

\section{Conclusion}

\lipsum[1]

%%%%%%%%%%%%%%
% References %
%%%%%%%%%%%%%%

\nocite{*}
\bibliographystyle{ieeetr}
\bibliography{main}

\end{document}
